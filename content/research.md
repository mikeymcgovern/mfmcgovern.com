---
title: "Research"
date: 2020-08-15T16:26:34-04:00
draft: false
comments: false
images:
---
Broadly speaking, I study the history of the human sciences, race, law, and technology. I am interested in how data—along with the technical concepts and practices that give them meaning—travel throughout society, make claims on people, and transform politics. As a scholar of U.S. history, I am particularly concerned with the overlap between the rise of computers and ongoing struggles against racism in an age of mass incarceration and rising inequality. My work brings historical epistemology and critical race theory together to inform a power-conscious reading of data and quantification in modern social life.

## Just in Numbers? Statistics and Civil Rights in Cold War America

{{< figure src="/images/deposition-notes.jpg" attr="From the David C. Baldus Papers, M.E. Grenander Department of Special Collections and Archives, University Libraries, University at Albany, State University of New York" class="big" >}}

My dissertation project, co-advised by [Erika Milam](https://history.princeton.edu/people/erika-lorraine-milam) and [Keith Wailoo](https://history.princeton.edu/people/keith-wailoo), charts the rise of statistical proof of racial discrimination in the U.S. legal system and narrates the litigation campaigns against employment discrimination and the death penalty from the perspective of the knowledge they produced. In the wake of _Brown v. Board of Education_ (1954), statistical disparities were increasingly cited as legal evidence of persistent, intentional segregation. Following the 1964 Civil Rights Act and the growing campaign to abolish capital punishment, advocates began wielding statistical disparities with new force and urgency, venturing legal arguments about the discriminatory effects of face-neutral policies. Despite strong inroads made during the 1970s, by the end of the 1980s a series of Supreme Court rulings reflecting a conservative realignment of American politics diminished the import of statistics—ironically, at the height of “information age” enthusiasm.

Focusing on collaborations between quantitative social scientists and civil rights organizations like the NAACP Legal Defense Fund, the American Civil Liberties Union, and the Equal Employment Opportunity Commission, my dissertation offers a new look at the interface between experts, litigators, and domestic policymakers during the late Cold War. I argue that debates over the limitations of statistics shored up commitments to colorblind justice and legitimated the structural inequality targeted by progressive lawyers. By providing a historical account of the opportunities and hazards of quantitative frameworks for racial justice, I hope to better inform discussions about discrimination in an era of big data and machine learning.


## Other projects

### Mainframes and Mendelians

{{< figure src="https://wellcomelibrary.org/content/images/23288/34272" attr="Image courtesy of the Wellcome Library" class="right" >}}

My MPhil dissertation, completed in the Department of History and Philosophy of Science at the University of Cambridge in 2014, explores the transnational history of computational gene mapping in the years before the Human Genome Project. Utilizing the papers of celebrated geneticist [Victor McKusick](https://medicalarchives.jhmi.edu:8443/papers/mckusick.html) and forgotten pioneer of statistical genetic mapping [James Renwick](https://wellcomelibrary.org/collections/digital-collections/makers-of-modern-genetics/digitised-archives/james-renwick/), I unpack how their momentous collaboration foundered on commitments to different models of labor, intellectual property, and statistical methodology. I've also worked extensively on another of McKusick's computational projects, _Mendelian Inheritance in Man_, documented in [this blog post](https://historyofknowledge.net/2018/05/07/taking-human-genetics-digital/).

### Amphibian _a priori_

{{< figure src="https://upload.wikimedia.org/wikipedia/commons/c/c0/Lettvin_Pitts.jpg" attr="Image from Wikimedia Commons" class="right" >}}

What makes one research program croak, and another purr? Touted as the origin point of second-order cybernetics, the 1959 paper “What the Frog’s Eye Tells the Frog’s Brain” emerged from attempts by MIT’s Warren McCulloch, Jerome Lettvin, and others to apply cybernetic logic to living brains. It claimed that fibers in the frog optic nerve were coded to relay distinct signals to the brain, each having one of a variety of “filters” tailored to the survival needs of the frog. This interpretation was as controversial as the experiment underlying it was capricious; only Lettvin’s sensitive hand could reproduce results, and the group largely abandoned the research. At the same time, David Hubel and Torsten Wiesel relocated from Johns Hopkins across the river to Harvard Medical School. Using a similar setup for the cat, they had just published a paper showing direction-specific “receptive fields” in single neurons of the cortex. This became the basis for studies of binocular vision that won them the 1981 Nobel Prize for Physiology or Medicine. Through close reading of the published literature and engagement with oral histories and material from McCulloch’s and Lettvin's archives, I argue that although they diverged substantially, these research programs were seen as complementary, particularly in early artificial intelligence research. While the cybernetics group invoked images of mental hardwiring, the Harvard team appealed to higher-order cognition. Such disciplinary distinctions, I suggest, reflect contests within the brain sciences over the character of liberal subjectivity in Cold War America. Draft available by request. I've started further work on Lettvin's later career, notably the influence of his [1967 debate on LSD with Timothy Leary](http://www.openculture.com/2014/09/the-historic-lsd-debate-at-mit.html) on the public perception of the mind sciences.
